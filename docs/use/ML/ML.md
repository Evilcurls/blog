# Machine Learning

# 基础

## 目标

 机器学习的目标就是拟合一个函数能够完成预测问题

三要素：模型 学习准则 优化算法

学习算法一般分为三种

​	监督学习

​		回归问题	标签y是连续值	f(x,θ)输出也是连续值

​		分类问题	y是离散值

​		结构化学习问题	实质是特殊分类问题 标签Y通常是结构化的对象 序列 /树/图之类的

​	

​	无监督学习:从不包含目标标签的训练样本中自动学到有价值的信息

​		聚类 	密度估计	特征学习 	降维

​	强化学习：通过交互来学习的机器学习算法 	在和环境的交互中不断学习并且调整策略

### 特征表示

​	首先机器学习核心应该是数学运算，所以要把各种特征转化为向量来表示，方便计算

​		图像：M*N的图像 变为 M * N维的向量，每维的数值是灰度，如果是彩色图就RGB三层叠加

​		文本： 简单方式是用词袋（Bag of words)但好像由于不考虑词序，不能精确表示文本信息

​		特征学习 又称表示学习   让机器自动学习出有效特征

​				方式：用人为设计准则  通过准则选出需要的特征

### **梯度消失（Vanishing Gradient）**

#### **定义**

在反向传播时，梯度值随着网络层数增加呈指数级衰减，导致浅层（靠近输入层）的权重几乎不更新，训练停滞



 

**梯度爆炸（Exploding Gradient）**

#### **定义**

在反向传播时，梯度值随着网络层数增加呈指数级增长，导致权重更新幅度过大，模型参数发散（NaN 或溢出）。

## 数据增广

扩充数据集：比如对已有图片加入噪声 镜像旋转 

​			可以遇到更加多样化的情况提升降低过拟合风险

## 微调

通过大数据集训练出来的模型用在小数据集上

因为大的数据集训练出来的初始模型泛化能力一般会强一点，所以用来初始化参数（反正肯定比随机设参数强）

那么在复制参数的时候，是不是一模一样复制呢：在神经

### 两条定律

没有免费午餐定律：炉石传说没有完爆

奥卡姆剃刀原理：简单的模型泛化能力更强	没必要不要增实体

# 各种问题

## 一元

predict	

![0](一元模型三要素.png)

​	在计算损失函数的时候，loss=yi-(wxi+b)，由于只是把标签值yi和输入值xi带入了，所以loss是关于两个参数的方程，可以计算最小值梯度等问题，当使用线性模型的时候会出现离群点，损失函数可以换成别的函数均方误差受到的影响太大

# 二分类问题

​	样本空间中找到一个超平面 它将特征空间一分为二 一半是我愿意 一半是我拒

![sigmoid函数](sigmoid函数.png)

​	在分类问题中不能用线性模型的输出，或者说不能直接用，因为一般线性模型的输出是连续的，所以要将线性模型的输出使用sigmoid函数转化为概率值	 把一整段连续值挤压到空间[0,1]，赋予概率值，0或1哪个超过0.5就分类结果是哪个

sigmoid函数=$ \frac{1} {1+e^{-x}} $

 ![二分类问题的步骤](二分类问题.png)

​	像这样的思想可以推广到多分类问题，就像之前那个手写体数字识别，就是用softmax激活函数完成的多分类问题

​	其中softmax=$\frac {e^x_i} {\sum_i {e^{xi}}}$

## SVM 支持向量机

模型＆目的：找一个超平面把标记了正负的样本空间里的样本给一分为二   就是之前的二分类问题，有一个名词叫支持向量，表示距离超平面最近的点，一共有两个，超平面移动到这两个点形成的平面分别叫正负超平面

如果一次性将所有的点都分开，那么叫做硬间隔，但尽善尽美的事情很少，往往最优的超平面会分出绝大多数的点，但是有那么一两个

点会分隔异常，所以此时把这些异常的点用损失因子记录，通过考虑损失因子和间隔的收益，这种方法叫做软间隔

学习准则：最大化间隔    间隔为2/|w|		所以是argmin|w|^2

优化策略：凸二次优化

问题：超平面又支持向量决定，支持向量又很少，导致解具有稀疏性



# 神经网络

​	神经元是最小单位，有轴突和树突，树突用于获取信息，轴突用于发送信息，神经元只有两种状态 ，激活和抑制

​	激活函数在模拟神经元的触发机制，当满足某种条件变成激活状态，

## sigmoid函数

​	长得像S型的函数  (两端饱和函数)一般有logstic 和 tanh

​	logstic函数的长这样   $\frac 1 {1+exp(-x)}$

​	tanh函数长这样  $\frac {exp(x)-exp(-x)} {exp(x)+exp(-x)}$





![ ](logistic&tanh.png)

logstic函数优势在于简单，劣势在于没有是非零中心化的，会影响后来的神经元，有偏置偏移

## Relu函数

长相：$f(x)=\begin{cases}1     x\ge 0 ，0      x<0 \end{cases}$

优点：计算高效，兴奋程度高

缺点：ReLU函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率． 此外，ReLU神经元在训练时比较容易“死亡”．在训 练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU神经元在 所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0， 在以后的训练过程中永远不能被激活．这种现象称为死亡ReLU问题

有好几种变种去解决这些问题：泄露的relu(leaky relu) 带参数的relu(parametric Relu) 还有好多什么ELU，什么GELU

# 网络结构

### 前馈神经网络

就是拟合一个函数，有向无环，单向传播

![](前馈神经网络.png)

上图示例就是将输入x输入到神经网络$\phi (x)$ 中去，分类器的参数是一堆$\theta$ ,在通过比如softmax分类器 对$\phi (x)$的输出数据分类

# 卷积神经网络

假设我直接将一个100*100的图片输入到全连接网络

假设有N个神经元，那么在后续的全连接层中1.输入参数过多不好计算 2.会丢失某些图像特征

所以卷积应运而生：

1.部分连接：

​     在全连接层中，第二层的任意神经元都要连接前面所有的神经元，那么就需（要维数*N+维数）个参数，但在卷积神经网络中，第二层中的所有神经元都各司其职，只与前一层的部分神经元连接

2.权重共享：

​	虽然部分连接决定了某些神经元去处理图像的不同区域，但是比如人脸识别这样的工作，人脸可能在上下左右的任意地方出现，所以可以用一套参数完成任务

