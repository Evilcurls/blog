# Machine Learning

# 基础

## 目标

 机器学习的目标就是拟合一个函数能够完成预测问题

三要素：模型 学习准则 优化算法

学习算法一般分为三种

​	监督学习

​		回归问题	标签y是连续值	f(x,θ)输出也是连续值

​		分类问题	y是离散值

​		结构化学习问题	实质是特殊分类问题 标签Y通常是结构化的对象 序列 /树/图之类的

​	

​	无监督学习:从不包含目标标签的训练样本中自动学到有价值的信息

​		聚类 	密度估计	特征学习 	降维

​	强化学习：通过交互来学习的机器学习算法 	在和环境的交互中不断学习并且调整策略

### 特征表示

​	首先机器学习核心应该是数学运算，所以要把各种特征转化为向量来表示，方便计算

​		图像：M*N的图像 变为 M * N维的向量，每维的数值是灰度，如果是彩色图就RGB三层叠加

​		文本： 简单方式是用词袋（Bag of words)但好像由于不考虑词序，不能精确表示文本信息

​		特征学习 又称表示学习   让机器自动学习出有效特征

​				方式：用人为设计准则  通过准则选出需要的特征

### 两条定律

没有免费午餐定律：炉石传说没有完爆

奥卡姆剃刀原理：简单的模型泛化能力更强	没必要不要增实体

# 各种问题

## 一元

predict	

![0](一元模型三要素.png)

​	在计算损失函数的时候，loss=yi-(wxi+b)，由于只是把标签值yi和输入值xi带入了，所以loss是关于两个参数的方程，可以计算最小值梯度等问题，当使用线性模型的时候会出现离群点，损失函数可以换成别的函数均方误差受到的影响太大

# 二分类问题

​	样本空间中找到一个超平面 它将特征空间一分为二 一半是我愿意 一半是我拒

![sigmoid函数](sigmoid函数.png)

​	在分类问题中不能用线性模型的输出，或者说不能直接用，因为一般线性模型的输出是连续的，所以要将线性模型的输出使用sigmoid函数转化为概率值	 把一整段连续值挤压到空间[0,1]，赋予概率值，0或1哪个超过0.5就分类结果是哪个

sigmoid函数=$ \frac{1} {1+e^{-x}} $

 ![二分类问题的步骤](二分类问题.png)

​	像这样的思想可以推广到多分类问题，就像之前那个手写体数字识别，就是用softmax激活函数完成的多分类问题

​	其中softmax=$\frac {e^x_i} {\sum_i {e^{xi}}}$